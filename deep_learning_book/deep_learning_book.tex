\batchmode
\documentclass[a4paper,]{article}
\usepackage{calc}
\usepackage[utf8]{inputenc}
\usepackage{pdfpages}
\usepackage[pdfpagelabels]{hyperref}
\usepackage{bookmark}

% Normal size 1.1, offset=0cm -2cm

\begin{document}
\renewcommand{\thepage}{\roman{page}}\setcounter{page}{0}
\includepdfmerge[fitpaper=true,rotateoversize=true,scale=1.17,offset=0cm -2cm]{TOC.pdf,-}
\includepdfmerge[fitpaper=true,rotateoversize=true,scale=1.17,offset=0cm -2cm]{acknowledgements.pdf,-}
\includepdfmerge[fitpaper=true,rotateoversize=true,scale=1.17,offset=0cm -2cm]{notation.pdf,-}
\renewcommand{\thepage}{\arabic{page}}\setcounter{page}{1}
\includepdfmerge[fitpaper=true,rotateoversize=true,scale=1.17,offset=0cm -2cm]{intro.pdf,-}
\includepdfmerge[fitpaper=true,rotateoversize=true,scale=1.17,offset=0cm -2cm]{part_basics.pdf,-}
\includepdfmerge[fitpaper=true,rotateoversize=true,scale=1.17,offset=0cm -2cm]{linear_algebra.pdf,-}
\includepdfmerge[fitpaper=true,rotateoversize=true,scale=1.17,offset=0cm -2cm]{prob.pdf,-}
\includepdfmerge[fitpaper=true,rotateoversize=true,scale=1.17,offset=0cm -2cm]{numerical.pdf,-}
\includepdfmerge[fitpaper=true,rotateoversize=true,scale=1.17,offset=0cm -2cm]{ml.pdf,-}                % Chapter 5
\includepdfmerge[fitpaper=true,rotateoversize=true,scale=1.17,offset=0cm -2cm]{part_practical.pdf,-}    % Part II
\includepdfmerge[fitpaper=true,rotateoversize=true,scale=1.17,offset=0cm -2cm]{mlp.pdf,-}
\includepdfmerge[fitpaper=true,rotateoversize=true,scale=1.17,offset=0cm -2cm]{regularization.pdf,-}
\includepdfmerge[fitpaper=true,rotateoversize=true,scale=1.17,offset=0cm -2cm]{optimization.pdf,-}
\includepdfmerge[fitpaper=true,rotateoversize=true,scale=1.17,offset=0cm -2cm]{convnets.pdf,-}
\includepdfmerge[fitpaper=true,rotateoversize=true,scale=1.17,offset=0cm -2cm]{rnn.pdf,-}
\includepdfmerge[fitpaper=true,rotateoversize=true,scale=1.17,offset=0cm -2cm]{guidelines.pdf,-}
\includepdfmerge[fitpaper=true,rotateoversize=true,scale=1.17,offset=0cm -2cm]{applications.pdf,-}
\includepdfmerge[fitpaper=true,rotateoversize=true,scale=1.17,offset=0cm -2cm]{part_research.pdf,-}
\includepdfmerge[fitpaper=true,rotateoversize=true,scale=1.17,offset=0cm -2cm]{linear_factors.pdf,-}
\includepdfmerge[fitpaper=true,rotateoversize=true,scale=1.17,offset=0cm -2cm]{autoencoders.pdf,-}
\includepdfmerge[fitpaper=true,rotateoversize=true,scale=1.17,offset=0cm -2cm]{representation.pdf,-}
\includepdfmerge[fitpaper=true,rotateoversize=true,scale=1.17,offset=0cm -2cm]{graphical_models.pdf,-}
\includepdfmerge[fitpaper=true,rotateoversize=true,scale=1.17,offset=0cm -2cm]{monte_carlo.pdf,-}
\includepdfmerge[fitpaper=true,rotateoversize=true,scale=1.17,offset=0cm -2cm]{partition.pdf,-}
\includepdfmerge[fitpaper=true,rotateoversize=true,scale=1.17,offset=0cm -2cm]{inference.pdf,-}
\includepdfmerge[fitpaper=true,rotateoversize=true,scale=1.17,offset=0cm -2cm]{generative_models.pdf,-}
\includepdfmerge[fitpaper=true,rotateoversize=true,scale=1.17,offset=0cm -2cm]{bib.pdf,-}
\includepdfmerge[fitpaper=true,rotateoversize=true,scale=1.17,offset=0cm -2cm]{index-.pdf,-}

\bookmark[page=9,view={XYZ 0 \calc{\paperheight} null},level=1]{Website}
\bookmark[page=10,view={XYZ 0 \calc{\paperheight} null},level=1]{Acknowledgments}
\bookmark[page=13,view={XYZ 0 \calc{\paperheight} null},level=1]{Notation}
\bookmark[page=17,view={XYZ 0 \calc{\paperheight} null},level=1]{1 Introduction}
\bookmark[page=24,view={XYZ 0 \calc{\paperheight} null},level=2]{1.1 Who Should Read This Book?}
\bookmark[page=28,view={XYZ 0 \calc{\paperheight} null},level=2]{1.2 Historical Trends in Deep Learning}
\bookmark[page=43,view={XYZ 0 \calc{\paperheight} null},level=1]{I Applied Math and Machine Learning Basics}
\bookmark[page=45,view={XYZ 0 \calc{\paperheight} null},level=1]{2 Linear Algebra}
\bookmark[page=45,view={XYZ 0 \calc{\paperheight} null},level=2]{2.1 Scalars, Vectors, Matrices and Tensors}
\bookmark[page=48,view={XYZ 0 \calc{\paperheight} null},level=2]{2.2 Multiplying Matrices and Vectors}
\bookmark[page=50,view={XYZ 0 \calc{\paperheight} null},level=2]{2.3 Identity and Inverse Matrices}
\bookmark[page=51,view={XYZ 0 \calc{\paperheight} null},level=2]{2.4 Linear Dependence and Span}
\bookmark[page=53,view={XYZ 0 \calc{\paperheight} null},level=2]{2.5 Norms}
\bookmark[page=54,view={XYZ 0 \calc{\paperheight} null},level=2]{2.6 Special Kinds of Matrices and Vectors}
\bookmark[page=56,view={XYZ 0 \calc{\paperheight} null},level=2]{2.7 Eigendecomposition}
\bookmark[page=58,view={XYZ 0 \calc{\paperheight} null},level=2]{2.8 Singular Value Decomposition}
\bookmark[page=59,view={XYZ 0 \calc{\paperheight} null},level=2]{2.9 The Moore-Penrose Pseudoinverse}
\bookmark[page=60,view={XYZ 0 \calc{\paperheight} null},level=2]{2.10 The Trace Operator}
\bookmark[page=61,view={XYZ 0 \calc{\paperheight} null},level=2]{2.11 The Determinant}
\bookmark[page=61,view={XYZ 0 \calc{\paperheight} null},level=2]{2.12 Example: Principal Components Analysis}
\bookmark[page=67,view={XYZ 0 \calc{\paperheight} null},level=1]{3 Probability and Information Theory}
\bookmark[page=68,view={XYZ 0 \calc{\paperheight} null},level=2]{3.1 Why Probability?}
\bookmark[page=70,view={XYZ 0 \calc{\paperheight} null},level=2]{3.2 Random Variables}
\bookmark[page=70,view={XYZ 0 \calc{\paperheight} null},level=2]{3.3 Probability Distributions}
\bookmark[page=72,view={XYZ 0 \calc{\paperheight} null},level=2]{3.4 Marginal Probability}
\bookmark[page=73,view={XYZ 0 \calc{\paperheight} null},level=2]{3.5 Conditional Probability}
\bookmark[page=73,view={XYZ 0 \calc{\paperheight} null},level=2]{3.6 The Chain Rule of Conditional Probabilities}
\bookmark[page=74,view={XYZ 0 \calc{\paperheight} null},level=2]{3.7 Independence and Conditional Independence}
\bookmark[page=74,view={XYZ 0 \calc{\paperheight} null},level=2]{3.8 Expectation, Variance and Covariance}
\bookmark[page=76,view={XYZ 0 \calc{\paperheight} null},level=2]{3.9 Common Probability Distributions}
\bookmark[page=81,view={XYZ 0 \calc{\paperheight} null},level=2]{3.10 Useful Properties of Common Functions}
\bookmark[page=84,view={XYZ 0 \calc{\paperheight} null},level=2]{3.11 Bayes' Rule}
\bookmark[page=85,view={XYZ 0 \calc{\paperheight} null},level=2]{3.12 Technical Details of Continuous Variables}
\bookmark[page=87,view={XYZ 0 \calc{\paperheight} null},level=2]{3.13 Information Theory}
\bookmark[page=89,view={XYZ 0 \calc{\paperheight} null},level=2]{3.14 Structured Probabilistic Models}
\bookmark[page=94,view={XYZ 0 \calc{\paperheight} null},level=1]{4 Numerical Computation}
\bookmark[page=94,view={XYZ 0 \calc{\paperheight} null},level=2]{4.1 Overflow and Underflow}
\bookmark[page=96,view={XYZ 0 \calc{\paperheight} null},level=2]{4.2 Poor Conditioning}
\bookmark[page=96,view={XYZ 0 \calc{\paperheight} null},level=2]{4.3 Gradient-Based Optimization}
\bookmark[page=107,view={XYZ 0 \calc{\paperheight} null},level=2]{4.4 Constrained Optimization}
\bookmark[page=110,view={XYZ 0 \calc{\paperheight} null},level=2]{4.5 Example: Linear Least Squares}
\bookmark[page=112,view={XYZ 0 \calc{\paperheight} null},level=1]{5 Machine Learning Basics}
\bookmark[page=113,view={XYZ 0 \calc{\paperheight} null},level=2]{5.1 Learning Algorithms}
\bookmark[page=124,view={XYZ 0 \calc{\paperheight} null},level=2]{5.2 Capacity, Overfitting and Underfitting}
\bookmark[page=134,view={XYZ 0 \calc{\paperheight} null},level=2]{5.3 Hyperparameters and Validation Sets}
\bookmark[page=136,view={XYZ 0 \calc{\paperheight} null},level=2]{5.4 Estimators, Bias and Variance}
\bookmark[page=145,view={XYZ 0 \calc{\paperheight} null},level=2]{5.5 Maximum Likelihood Estimation}
\bookmark[page=149,view={XYZ 0 \calc{\paperheight} null},level=2]{5.6 Bayesian Statistics}
\bookmark[page=153,view={XYZ 0 \calc{\paperheight} null},level=2]{5.7 Supervised Learning Algorithms}
\bookmark[page=158,view={XYZ 0 \calc{\paperheight} null},level=2]{5.8 Unsupervised Learning Algorithms}
\bookmark[page=165,view={XYZ 0 \calc{\paperheight} null},level=2]{5.9 Stochastic Gradient Descent}
\bookmark[page=167,view={XYZ 0 \calc{\paperheight} null},level=2]{5.10 Building a Machine Learning Algorithm}
\bookmark[page=168,view={XYZ 0 \calc{\paperheight} null},level=2]{5.11 Challenges Motivating Deep Learning}
\bookmark[page=178,view={XYZ 0 \calc{\paperheight} null},level=1]{II Deep Networks: Modern Practices}
\bookmark[page=180,view={XYZ 0 \calc{\paperheight} null},level=1]{6 Deep Feedforward Networks}
\bookmark[page=183,view={XYZ 0 \calc{\paperheight} null},level=2]{6.1 Example: Learning XOR}
\bookmark[page=188,view={XYZ 0 \calc{\paperheight} null},level=2]{6.2 Gradient-Based Learning}
\bookmark[page=203,view={XYZ 0 \calc{\paperheight} null},level=2]{6.3 Hidden Units}
\bookmark[page=209,view={XYZ 0 \calc{\paperheight} null},level=2]{6.4 Architecture Design}
\bookmark[page=216,view={XYZ 0 \calc{\paperheight} null},level=2]{6.5 Back-Propagation and Other Differentiation Algorithms}
\bookmark[page=236,view={XYZ 0 \calc{\paperheight} null},level=2]{6.6 Historical Notes}
\bookmark[page=240,view={XYZ 0 \calc{\paperheight} null},level=1]{7 Regularization for Deep Learning}
\bookmark[page=242,view={XYZ 0 \calc{\paperheight} null},level=2]{7.1 Parameter Norm Penalties}
\bookmark[page=249,view={XYZ 0 \calc{\paperheight} null},level=2]{7.2 Norm Penalties as Constrained Optimization}
\bookmark[page=251,view={XYZ 0 \calc{\paperheight} null},level=2]{7.3 Regularization and Under-Constrained Problems}
\bookmark[page=252,view={XYZ 0 \calc{\paperheight} null},level=2]{7.4 Dataset Augmentation}
\bookmark[page=254,view={XYZ 0 \calc{\paperheight} null},level=2]{7.5 Noise Robustness}
\bookmark[page=256,view={XYZ 0 \calc{\paperheight} null},level=2]{7.6 Semi-Supervised Learning}
\bookmark[page=257,view={XYZ 0 \calc{\paperheight} null},level=2]{7.7 Multitask Learning}
\bookmark[page=257,view={XYZ 0 \calc{\paperheight} null},level=2]{7.8 Early Stopping}
\bookmark[page=265,view={XYZ 0 \calc{\paperheight} null},level=2]{7.9 Parameter Tying and Parameter Sharing}
\bookmark[page=267,view={XYZ 0 \calc{\paperheight} null},level=2]{7.10 Sparse Representations}
\bookmark[page=269,view={XYZ 0 \calc{\paperheight} null},level=2]{7.11 Bagging and Other Ensemble Methods}
\bookmark[page=271,view={XYZ 0 \calc{\paperheight} null},level=2]{7.12 Dropout}
\bookmark[page=281,view={XYZ 0 \calc{\paperheight} null},level=2]{7.13 Adversarial Training}
\bookmark[page=283,view={XYZ 0 \calc{\paperheight} null},level=2]{7.14 Tangent Distance, Tangent Prop and Manifold Tangent Classifier}
\bookmark[page=287,view={XYZ 0 \calc{\paperheight} null},level=1]{8 Optimization for Training Deep Models}
\bookmark[page=288,view={XYZ 0 \calc{\paperheight} null},level=2]{8.1 How Learning Differs from Pure Optimization}
\bookmark[page=295,view={XYZ 0 \calc{\paperheight} null},level=2]{8.2 Challenges in Neural Network Optimization}
\bookmark[page=306,view={XYZ 0 \calc{\paperheight} null},level=2]{8.3 Basic Algorithms}
\bookmark[page=312,view={XYZ 0 \calc{\paperheight} null},level=2]{8.4 Parameter Initialization Strategies}
\bookmark[page=318,view={XYZ 0 \calc{\paperheight} null},level=2]{8.5 Algorithms with Adaptive Learning Rates}
\bookmark[page=323,view={XYZ 0 \calc{\paperheight} null},level=2]{8.6 Approximate Second-Order Methods}
\bookmark[page=329,view={XYZ 0 \calc{\paperheight} null},level=2]{8.7 Optimization Strategies and Meta-Algorithms}
\bookmark[page=342,view={XYZ 0 \calc{\paperheight} null},level=1]{9 Convolutional Networks}
\bookmark[page=343,view={XYZ 0 \calc{\paperheight} null},level=2]{9.1 The Convolution Operation}
\bookmark[page=345,view={XYZ 0 \calc{\paperheight} null},level=2]{9.2 Motivation}
\bookmark[page=351,view={XYZ 0 \calc{\paperheight} null},level=2]{9.3 Pooling}
\bookmark[page=355,view={XYZ 0 \calc{\paperheight} null},level=2]{9.4 Convolution and Pooling as an Infinitely Strong Prior}
\bookmark[page=358,view={XYZ 0 \calc{\paperheight} null},level=2]{9.5 Variants of the Basic Convolution Function}
\bookmark[page=368,view={XYZ 0 \calc{\paperheight} null},level=2]{9.6 Structured Outputs}
\bookmark[page=370,view={XYZ 0 \calc{\paperheight} null},level=2]{9.7 Data Types}
\bookmark[page=372,view={XYZ 0 \calc{\paperheight} null},level=2]{9.8 Efficient Convolution Algorithms}
\bookmark[page=372,view={XYZ 0 \calc{\paperheight} null},level=2]{9.9 Random or Unsupervised Features}
\bookmark[page=374,view={XYZ 0 \calc{\paperheight} null},level=2]{9.10 The Neuroscientific Basis for Convolutional Networks}
\bookmark[page=381,view={XYZ 0 \calc{\paperheight} null},level=2]{9.11 Convolutional Networks and the History of Deep Learning}
\bookmark[page=383,view={XYZ 0 \calc{\paperheight} null},level=1]{10 Sequence Modeling: Recurrent and Recursive Nets}
\bookmark[page=385,view={XYZ 0 \calc{\paperheight} null},level=2]{10.1 Unfolding Computational Graphs}
\bookmark[page=388,view={XYZ 0 \calc{\paperheight} null},level=2]{10.2 Recurrent Neural Networks}
\bookmark[page=404,view={XYZ 0 \calc{\paperheight} null},level=2]{10.3 Bidirectional RNNs}
\bookmark[page=406,view={XYZ 0 \calc{\paperheight} null},level=2]{10.4 Encoder-Decoder Sequence-to-Sequence Architectures}
\bookmark[page=408,view={XYZ 0 \calc{\paperheight} null},level=2]{10.5 Deep Recurrent Networks}
\bookmark[page=410,view={XYZ 0 \calc{\paperheight} null},level=2]{10.6 Recursive Neural Networks}
\bookmark[page=412,view={XYZ 0 \calc{\paperheight} null},level=2]{10.7 The Challenge of Long-Term Dependencies}
\bookmark[page=415,view={XYZ 0 \calc{\paperheight} null},level=2]{10.8 Echo State Networks}
\bookmark[page=418,view={XYZ 0 \calc{\paperheight} null},level=2]{10.9 Leaky Units and Other Strategies for Multiple Time Scales}
\bookmark[page=420,view={XYZ 0 \calc{\paperheight} null},level=2]{10.10 The Long Short-Term Memory and Other Gated RNNs}
\bookmark[page=424,view={XYZ 0 \calc{\paperheight} null},level=2]{10.11 Optimization for Long-Term Dependencies}
\bookmark[page=428,view={XYZ 0 \calc{\paperheight} null},level=2]{10.12 Explicit Memory}
\bookmark[page=432,view={XYZ 0 \calc{\paperheight} null},level=1]{11 Practical Methodology}
\bookmark[page=433,view={XYZ 0 \calc{\paperheight} null},level=2]{11.1 Performance Metrics}
\bookmark[page=436,view={XYZ 0 \calc{\paperheight} null},level=2]{11.2 Default Baseline Models}
\bookmark[page=437,view={XYZ 0 \calc{\paperheight} null},level=2]{11.3 Determining Whether to Gather More Data}
\bookmark[page=438,view={XYZ 0 \calc{\paperheight} null},level=2]{11.4 Selecting Hyperparameters}
\bookmark[page=447,view={XYZ 0 \calc{\paperheight} null},level=2]{11.5 Debugging Strategies}
\bookmark[page=451,view={XYZ 0 \calc{\paperheight} null},level=2]{11.6 Example: Multi-Digit Number Recognition}
\bookmark[page=454,view={XYZ 0 \calc{\paperheight} null},level=1]{12 Applications}
\bookmark[page=454,view={XYZ 0 \calc{\paperheight} null},level=2]{12.1 Large-Scale Deep Learning}
\bookmark[page=463,view={XYZ 0 \calc{\paperheight} null},level=2]{12.2 Computer Vision}
\bookmark[page=469,view={XYZ 0 \calc{\paperheight} null},level=2]{12.3 Speech Recognition}
\bookmark[page=472,view={XYZ 0 \calc{\paperheight} null},level=2]{12.4 Natural Language Processing}
\bookmark[page=489,view={XYZ 0 \calc{\paperheight} null},level=2]{12.5 Other Applications}
\bookmark[page=498,view={XYZ 0 \calc{\paperheight} null},level=1]{III Deep Learning Research}
\bookmark[page=501,view={XYZ 0 \calc{\paperheight} null},level=1]{13 Linear Factor Models}
\bookmark[page=502,view={XYZ 0 \calc{\paperheight} null},level=2]{13.1 Probabilistic PCA and Factor Analysis}
\bookmark[page=503,view={XYZ 0 \calc{\paperheight} null},level=2]{13.2 Independent Component Analysis (ICA)}
\bookmark[page=505,view={XYZ 0 \calc{\paperheight} null},level=2]{13.3 Slow Feature Analysis}
\bookmark[page=508,view={XYZ 0 \calc{\paperheight} null},level=2]{13.4 Sparse Coding}
\bookmark[page=512,view={XYZ 0 \calc{\paperheight} null},level=2]{13.5 Manifold Interpretation of PCA}
\bookmark[page=515,view={XYZ 0 \calc{\paperheight} null},level=1]{14 Autoencoders}
\bookmark[page=516,view={XYZ 0 \calc{\paperheight} null},level=2]{14.1 Undercomplete Autoencoders}
\bookmark[page=517,view={XYZ 0 \calc{\paperheight} null},level=2]{14.2 Regularized Autoencoders}
\bookmark[page=521,view={XYZ 0 \calc{\paperheight} null},level=2]{14.3 Representational Power, Layer Size and Depth}
\bookmark[page=522,view={XYZ 0 \calc{\paperheight} null},level=2]{14.4 Stochastic Encoders and Decoders}
\bookmark[page=523,view={XYZ 0 \calc{\paperheight} null},level=2]{14.5 Denoising Autoencoders}
\bookmark[page=529,view={XYZ 0 \calc{\paperheight} null},level=2]{14.6 Learning Manifolds with Autoencoders}
\bookmark[page=534,view={XYZ 0 \calc{\paperheight} null},level=2]{14.7 Contractive Autoencoders}
\bookmark[page=537,view={XYZ 0 \calc{\paperheight} null},level=2]{14.8 Predictive Sparse Decomposition}
\bookmark[page=538,view={XYZ 0 \calc{\paperheight} null},level=2]{14.9 Applications of Autoencoders}
\bookmark[page=540,view={XYZ 0 \calc{\paperheight} null},level=1]{15 Representation Learning}
\bookmark[page=542,view={XYZ 0 \calc{\paperheight} null},level=2]{15.1 Greedy Layer-Wise Unsupervised Pretraining}
\bookmark[page=550,view={XYZ 0 \calc{\paperheight} null},level=2]{15.2 Transfer Learning and Domain Adaptation}
\bookmark[page=555,view={XYZ 0 \calc{\paperheight} null},level=2]{15.3 Semi-Supervised Disentangling of Causal Factors}
\bookmark[page=560,view={XYZ 0 \calc{\paperheight} null},level=2]{15.4 Distributed Representation}
\bookmark[page=566,view={XYZ 0 \calc{\paperheight} null},level=2]{15.5 Exponential Gains from Depth}
\bookmark[page=568,view={XYZ 0 \calc{\paperheight} null},level=2]{15.6 Providing Clues to Discover Underlying Causes}
\bookmark[page=571,view={XYZ 0 \calc{\paperheight} null},level=1]{16 Structured Probabilistic Models for Deep Learning}
\bookmark[page=572,view={XYZ 0 \calc{\paperheight} null},level=2]{16.1 The Challenge of Unstructured Modeling}
\bookmark[page=576,view={XYZ 0 \calc{\paperheight} null},level=2]{16.2 Using Graphs to Describe Model Structure}
\bookmark[page=593,view={XYZ 0 \calc{\paperheight} null},level=2]{16.3 Sampling from Graphical Models}
\bookmark[page=595,view={XYZ 0 \calc{\paperheight} null},level=2]{16.4 Advantages of Structured Modeling}
\bookmark[page=595,view={XYZ 0 \calc{\paperheight} null},level=2]{16.5 Learning about Dependencies}
\bookmark[page=596,view={XYZ 0 \calc{\paperheight} null},level=2]{16.6 Inference and Approximate Inference}
\bookmark[page=597,view={XYZ 0 \calc{\paperheight} null},level=2]{16.7 The Deep Learning Approach to Structured Probabilistic Models}
\bookmark[page=603,view={XYZ 0 \calc{\paperheight} null},level=1]{17 Monte Carlo Methods}
\bookmark[page=603,view={XYZ 0 \calc{\paperheight} null},level=2]{17.1 Sampling and Monte Carlo Methods}
\bookmark[page=605,view={XYZ 0 \calc{\paperheight} null},level=2]{17.2 Importance Sampling}
\bookmark[page=608,view={XYZ 0 \calc{\paperheight} null},level=2]{17.3 Markov Chain Monte Carlo Methods}
\bookmark[page=612,view={XYZ 0 \calc{\paperheight} null},level=2]{17.4 Gibbs Sampling}
\bookmark[page=613,view={XYZ 0 \calc{\paperheight} null},level=2]{17.5 The Challenge of Mixing between Separated Modes}
\bookmark[page=619,view={XYZ 0 \calc{\paperheight} null},level=1]{18 Confronting the Partition Function}
\bookmark[page=620,view={XYZ 0 \calc{\paperheight} null},level=2]{18.1 The Log-Likelihood Gradient}
\bookmark[page=621,view={XYZ 0 \calc{\paperheight} null},level=2]{18.2 Stochastic Maximum Likelihood and Contrastive Divergence}
\bookmark[page=629,view={XYZ 0 \calc{\paperheight} null},level=2]{18.3 Pseudolikelihood}
\bookmark[page=631,view={XYZ 0 \calc{\paperheight} null},level=2]{18.4 Score Matching and Ratio Matching}
\bookmark[page=633,view={XYZ 0 \calc{\paperheight} null},level=2]{18.5 Denoising Score Matching}
\bookmark[page=634,view={XYZ 0 \calc{\paperheight} null},level=2]{18.6 Noise-Contrastive Estimation}
\bookmark[page=637,view={XYZ 0 \calc{\paperheight} null},level=2]{18.7 Estimating the Partition Function}
\bookmark[page=645,view={XYZ 0 \calc{\paperheight} null},level=1]{19 Approximate Inference}
\bookmark[page=647,view={XYZ 0 \calc{\paperheight} null},level=2]{19.1 Inference as Optimization}
\bookmark[page=648,view={XYZ 0 \calc{\paperheight} null},level=2]{19.2 Expectation Maximization}
\bookmark[page=649,view={XYZ 0 \calc{\paperheight} null},level=2]{19.3 MAP Inference and Sparse Coding}
\bookmark[page=652,view={XYZ 0 \calc{\paperheight} null},level=2]{19.4 Variational Inference and Learning}
\bookmark[page=664,view={XYZ 0 \calc{\paperheight} null},level=2]{19.5 Learned Approximate Inference}
\bookmark[page=667,view={XYZ 0 \calc{\paperheight} null},level=1]{20 Deep Generative Models}
\bookmark[page=667,view={XYZ 0 \calc{\paperheight} null},level=2]{20.1 Boltzmann Machines}
\bookmark[page=669,view={XYZ 0 \calc{\paperheight} null},level=2]{20.2 Restricted Boltzmann Machines}
\bookmark[page=673,view={XYZ 0 \calc{\paperheight} null},level=2]{20.3 Deep Belief Networks}
\bookmark[page=676,view={XYZ 0 \calc{\paperheight} null},level=2]{20.4 Deep Boltzmann Machines}
\bookmark[page=689,view={XYZ 0 \calc{\paperheight} null},level=2]{20.5 Boltzmann Machines for Real-Valued Data}
\bookmark[page=695,view={XYZ 0 \calc{\paperheight} null},level=2]{20.6 Convolutional Boltzmann Machines}
\bookmark[page=697,view={XYZ 0 \calc{\paperheight} null},level=2]{20.7 Boltzmann Machines for Structured or Sequential Outputs}
\bookmark[page=699,view={XYZ 0 \calc{\paperheight} null},level=2]{20.8 Other Boltzmann Machines}
\bookmark[page=700,view={XYZ 0 \calc{\paperheight} null},level=2]{20.9 Back-Propagation through Random Operations}
\bookmark[page=704,view={XYZ 0 \calc{\paperheight} null},level=2]{20.10 Directed Generative Nets}
\bookmark[page=723,view={XYZ 0 \calc{\paperheight} null},level=2]{20.11 Drawing Samples from Autoencoders}
\bookmark[page=726,view={XYZ 0 \calc{\paperheight} null},level=2]{20.12 Generative Stochastic Networks}
\bookmark[page=728,view={XYZ 0 \calc{\paperheight} null},level=2]{20.13 Other Generation Schemes}
\bookmark[page=729,view={XYZ 0 \calc{\paperheight} null},level=2]{20.14 Evaluating Generative Models}
\bookmark[page=732,view={XYZ 0 \calc{\paperheight} null},level=2]{20.15 Conclusion}
\bookmark[page=733,view={XYZ 0 \calc{\paperheight} null},level=1]{Bibliography}
\bookmark[page=790,view={XYZ 0 \calc{\paperheight} null},level=1]{Index}
\end{document}
